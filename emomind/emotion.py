# -*- coding: utf-8 -*-
"""emotion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rFCYZ9OCFdIXRUf6l_DRkwqy81tJo2N7
"""

import transformers
import torch
import numpy as np
from transformers import Wav2Vec2FeatureExtractor, WavLMForSequenceClassification
import torchaudio

device = 'cuda'
model_name = 'xbgoose/wavlm-base-speech-emotion-recognition-russian-dusha-finetuned'
feature_extractor_name = 'microsoft/wavlm-base'

feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(feature_extractor_name)
model = WavLMForSequenceClassification.from_pretrained(model_name).to(device)

label2emotion = {
    0: "нейтральное",
    1: "позитивное",
    2: "грустное",
    3: "злое",
    4: "другое"
}
model2data = {
    0:0,
    1:3,
    2:1,
    3:2,
    4:4
}

def get_emotion(path, model = model, tokenizer = feature_extractor):
    waveform, sample_rate = torchaudio.load(path, normalize=True)
    transform = torchaudio.transforms.Resample(sample_rate, 16000)
    data = transform(waveform)
    inputs = feature_extractor(
        data,
        sampling_rate=feature_extractor.sampling_rate,
        return_tensors="pt",
        padding=True,
        max_length=16000 * 10,
        truncation=True
    )
    logits = model(inputs['input_values'][0].cuda()).logits
    predictions = torch.argmax(logits, dim=-1)
    predicted_emotion = label2emotion[model2data[predictions.cpu().numpy()[0]]]

    return predicted_emotion