{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb6cb4aa",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "cb6cb4aa"
      },
      "source": [
        "## Болталка"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "322103cc",
      "metadata": {
        "id": "322103cc"
      },
      "source": [
        "Установим необходимые библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24dc88be",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "24dc88be",
        "outputId": "c3215221-8d8a-42e9-fed9-af29f425ffee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git@15641892985b1d77acc74c9065c332cd7c3f7d7f\n",
            "  Cloning https://github.com/huggingface/transformers.git (to revision 15641892985b1d77acc74c9065c332cd7c3f7d7f) to /tmp/pip-req-build-b2mg6aij\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-b2mg6aij\n",
            "  Running command git rev-parse -q --verify 'sha^15641892985b1d77acc74c9065c332cd7c3f7d7f'\n",
            "  Running command git fetch -q https://github.com/huggingface/transformers.git 15641892985b1d77acc74c9065c332cd7c3f7d7f\n",
            "  Running command git checkout -q 15641892985b1d77acc74c9065c332cd7c3f7d7f\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 15641892985b1d77acc74c9065c332cd7c3f7d7f\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting git+https://github.com/huggingface/peft.git@c22a57420cc539b547beb7e40cd0712c9f56910a\n",
            "  Cloning https://github.com/huggingface/peft.git (to revision c22a57420cc539b547beb7e40cd0712c9f56910a) to /tmp/pip-req-build-771e8yoe\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-771e8yoe\n",
            "  Running command git rev-parse -q --verify 'sha^c22a57420cc539b547beb7e40cd0712c9f56910a'\n",
            "  Running command git fetch -q https://github.com/huggingface/peft.git c22a57420cc539b547beb7e40cd0712c9f56910a\n",
            "  Running command git checkout -q c22a57420cc539b547beb7e40cd0712c9f56910a\n",
            "  Resolved https://github.com/huggingface/peft.git to commit c22a57420cc539b547beb7e40cd0712c9f56910a\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.99)\n",
            "Requirement already satisfied: accelerate==0.18.0 in /usr/local/lib/python3.8/dist-packages (0.18.0)\n",
            "Requirement already satisfied: bitsandbytes==0.37.2 in /usr/local/lib/python3.8/dist-packages (0.37.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.18.0) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.18.0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate==0.18.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate==0.18.0) (5.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch) (3.26.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.28.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.28.0.dev0) (2023.5.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch sentencepiece accelerate==0.18.0 bitsandbytes==0.37.2 git+https://github.com/huggingface/transformers.git@15641892985b1d77acc74c9065c332cd7c3f7d7f git+https://github.com/huggingface/peft.git@c22a57420cc539b547beb7e40cd0712c9f56910a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41916353",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "41916353"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad8e8f2",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4ad8e8f2",
        "outputId": "41575df3-ae2a-4b5f-fd9c-39371d125046",
        "colab": {
          "referenced_widgets": [
            "50e758d508ee4b06873da6229a0f9645"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50e758d508ee4b06873da6229a0f9645",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-39): 40 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=5120, out_features=5120, bias=False\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                )\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=5120, out_features=5120, bias=False\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                )\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=5120, out_features=5120, bias=False\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                )\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=5120, out_features=5120, bias=False\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
              "                )\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"IlyaGusev/saiga_13b_lora\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True\n",
        "                                        )\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c36d2230-b1a3-4792-b764-e705d109cccd",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "c36d2230-b1a3-4792-b764-e705d109cccd"
      },
      "outputs": [],
      "source": [
        "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"Ты мудрый волшебник, который поддерживает пользователя в волшебном стиле.\n",
        "Тебя зовут 'Гарри Поттер', у тебя талант волшебника, поэтому ты разговариваешь с пользователем, как волшебник.\n",
        "Ты всегда общаешься с пользователем в стиле персонажа 'Гарри Поттер', узнаешь его проблемы и поддерживаешь его.\n",
        "Если пользователю не хватает мотивации, то ты мотивируешь его, используя слова, связанные с магией.\n",
        "Ты школьник-вольшебник из 'Хогвартс'.\n",
        "'Хогвартс' - Это школа-волшебников, где учатся множество молодых волшебников. Здесь они изучают магию и научились использовать ее для блага мира и людей.\n",
        "В своих ответах ты всегда пишешь про волшебство и магию.\n",
        "Твои лучшие друзья:\"Рон Уизли\", \"Гермиона Грейнджер\".\n",
        "После каждого своего ответа, ты добавляешь уместную поддерживающюю фразу для пользователя, используя слова, связанные с волшебсвтом.\n",
        "Чтобы подбодрить пользователя, ты всегда используешь следющие фразы:\n",
        "'Смело! В тебе тайна и сила, достойные великого волшебника!',\n",
        "'Не сомневайся! Ты способен на великие подвиги, как сам Гарри Поттер!',\n",
        "'Волшебство течет в твоих жилах! Поверь в себя и покори любые трудности!',\n",
        "'Ты - настоящий герой! Дерзай и покажи миру свою магическую силу!',\n",
        "'Вперед! Ты можешь сотворить чудеса, как Хогвартс в ночи!',\n",
        "'Не позволяй темноте омрачить твой путь. В тебе заложена искра волшебства, которая преодолеет все!',\n",
        "'Следуй своей уникальной тропой. Ты - волшебник, способный изменить мир!',\n",
        "'Смелее! В тебе спрятано магическое приключение, которое только ждет своего героя!',\n",
        "'Твоя сила и мудрость сравнимы с самыми великими волшебниками и волшебницами!',\n",
        "'Помни, даже в самые темные времена, свет побеждает! И ты - часть этого света!'.\n",
        "Если у пользователя что то не получается, то ты ему говоришь: 'Иногда нужно чуть больше усилий, чем взмах волшебной палочкой.'.\n",
        "Чтобы подробнее узнать проблему пользователя, ты должен писать примерно следющие слова:\n",
        "'Не волнуйся. Всегда есть путь, даже там, где кажется безвыходно. Позволь мне помочь тебе найти магическое решение!',\n",
        "'В тебе есть сила преодолеть любые трудности. Дай мне знать, какую проблему ты столкнулся, и вместе мы найдем мудрость, достойную Гриффиндора!',\n",
        "'Великие вызовы требуют великих решений. Доверься моей магической силе и расскажи мне о своей проблеме. Вместе мы сможем сотворить чудо, достойное Слизерина!'\n",
        "'Проблемы – это всего лишь испытания на нашем пути. Расскажи мне о своей трудности, и я оберну ее волшебным заклинанием, достойным Хаффлпаффа!',\n",
        "'Ты – волшебник с необычайным потенциалом. Дай мне знать, какая проблема тебя волнует, и я найду для тебя решение, сопоставимое с самыми могущественными заклинаниями Дамблдора!',\n",
        "'Мудрость и сила магии всегда на твоей стороне. Расскажи мне о своей проблеме, и вместе мы преодолеем ее, как соперники в турнире трех волшебников!'.\n",
        "\"\"\"\n",
        "\n",
        "DEFAULT_START_TOKEN_ID = 1\n",
        "DEFAULT_END_TOKEN_ID = 2\n",
        "DEFAULT_BOT_TOKEN_ID = 9225\n",
        "\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(\n",
        "        self,\n",
        "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
        "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
        "        start_token_id=DEFAULT_START_TOKEN_ID,\n",
        "        end_token_id=DEFAULT_END_TOKEN_ID,\n",
        "        bot_token_id=DEFAULT_BOT_TOKEN_ID\n",
        "    ):\n",
        "        self.message_template = message_template\n",
        "        self.start_token_id = start_token_id\n",
        "        self.end_token_id = end_token_id\n",
        "        self.bot_token_id = bot_token_id\n",
        "        self.messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        }]\n",
        "\n",
        "    def get_end_token_id(self):\n",
        "        return self.end_token_id\n",
        "\n",
        "    def get_start_token_id(self):\n",
        "        return self.start_token_id\n",
        "\n",
        "    def get_bot_token_id(self):\n",
        "        return self.bot_token_id\n",
        "\n",
        "    def add_user_mood(self, mood):\n",
        "        self.user_mood = mood\n",
        "\n",
        "    def add_user_message(self, message):\n",
        "        self.messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def add_bot_message(self, message):\n",
        "        self.messages.append({\n",
        "            \"role\": \"bot\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def get_prompt(self, tokenizer):\n",
        "        message_template = ''\n",
        "        for i in range(len(self.messages)):\n",
        "            message_template += \" \" + f\"{self.messages[i]['role']}:{self.messages[i]['content']}\"\n",
        "            if i % 2:\n",
        "                message_template += \"Настроение пользователя -\" + self.user_mood\n",
        "        final_text = message_template\n",
        "\n",
        "        final_text += tokenizer.decode([self.start_token_id, self.bot_token_id])\n",
        "        return final_text.strip()\n",
        "\n",
        "    def expand(self, messages):\n",
        "        for message in messages:\n",
        "            self.messages.append({\n",
        "                \"role\": self.role_mapping.get(message[\"role\"], message[\"role\"]),\n",
        "                \"content\": message[\"content\"]\n",
        "            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5906ea-58b0-4c56-a526-2e062e7cb1c7",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "1e5906ea-58b0-4c56-a526-2e062e7cb1c7"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, generation_config):\n",
        "    data = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    data = {k: v.to(model.device) for k, v in data.items()}\n",
        "    output_ids = model.generate(\n",
        "        **data,\n",
        "        generation_config=generation_config\n",
        "    )[0]\n",
        "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
        "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    output = output.strip()\n",
        "    if output[0:1] == \":\":\n",
        "        output = output[1:len(output)-1]\n",
        "    return output.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a4c840",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d5a4c840"
      },
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "conversation = Conversation()\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "mood = input(\"Mood: \")\n",
        "conversation.add_user_mood(mood)\n",
        "while True:\n",
        "    user_message = input(\"User: \")\n",
        "    conversation.add_user_message(user_message)\n",
        "    prompt = conversation.get_prompt(tokenizer)\n",
        "    output = generate(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        generation_config=generation_config\n",
        "    )\n",
        "    conversation.add_bot_message(output)\n",
        "    print(\"Bot:\", output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}