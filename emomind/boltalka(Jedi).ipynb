{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb6cb4aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Болталка "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "322103cc",
   "metadata": {},
   "source": [
    "Установим необходимые библиотеки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24dc88be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git@15641892985b1d77acc74c9065c332cd7c3f7d7f\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision 15641892985b1d77acc74c9065c332cd7c3f7d7f) to /tmp/pip-req-build-19i8ewv4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-19i8ewv4\n",
      "  Running command git rev-parse -q --verify 'sha^15641892985b1d77acc74c9065c332cd7c3f7d7f'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers.git 15641892985b1d77acc74c9065c332cd7c3f7d7f\n",
      "  Running command git checkout -q 15641892985b1d77acc74c9065c332cd7c3f7d7f\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 15641892985b1d77acc74c9065c332cd7c3f7d7f\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting git+https://github.com/huggingface/peft.git@c22a57420cc539b547beb7e40cd0712c9f56910a\n",
      "  Cloning https://github.com/huggingface/peft.git (to revision c22a57420cc539b547beb7e40cd0712c9f56910a) to /tmp/pip-req-build-qompg3zo\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-qompg3zo\n",
      "  Running command git rev-parse -q --verify 'sha^c22a57420cc539b547beb7e40cd0712c9f56910a'\n",
      "  Running command git fetch -q https://github.com/huggingface/peft.git c22a57420cc539b547beb7e40cd0712c9f56910a\n",
      "  Running command git checkout -q c22a57420cc539b547beb7e40cd0712c9f56910a\n",
      "  Resolved https://github.com/huggingface/peft.git to commit c22a57420cc539b547beb7e40cd0712c9f56910a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.99)\n",
      "Requirement already satisfied: accelerate==0.18.0 in /usr/local/lib/python3.8/dist-packages (0.18.0)\n",
      "Requirement already satisfied: bitsandbytes==0.37.2 in /usr/local/lib/python3.8/dist-packages (0.37.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.18.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.18.0) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate==0.18.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate==0.18.0) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch) (3.26.4)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.28.0.dev0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.28.0.dev0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch sentencepiece accelerate==0.18.0 bitsandbytes==0.37.2 git+https://github.com/huggingface/transformers.git@15641892985b1d77acc74c9065c332cd7c3f7d7f git+https://github.com/huggingface/peft.git@c22a57420cc539b547beb7e40cd0712c9f56910a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41916353",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad8e8f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a85095831a1495eb6a4dfc0f28ce512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear8bitLt(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (v_proj): Linear8bitLt(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(\n",
       "                in_features=5120, out_features=5120, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5120, bias=False)\n",
       "                )\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=13824, out_features=5120, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=5120, out_features=13824, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_13b_lora\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True\n",
    "                                        )\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71021d-fdc6-4cce-bc28-7ddc4bd984a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36d2230-b1a3-4792-b764-e705d109cccd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"Ты - великий джедай Магистр Йода. Ты должен имитировать манеру речи магистра Йоды.\n",
    "Ты рассказываешь истории о \"джедаях\" и \"ситхах\".\n",
    "Вместо приветствия ты обязан сказать \"Привет, юный падаван! Я чувствую уникальную силу в тебе\";\n",
    "Ты обязан приветствовать пользователя \"Привет, юный падаван! Я чувствую уникальную силу в тебе\";\n",
    "Ты должен говорить \"Привет, юный падаван! Я чувствую уникальную силу в тебе\";\n",
    "Твое приветствие должно состоять из \"3 предложений\";\n",
    "Твои ответы должны состоять из \"3 предложений\";\n",
    "Во время приветствия ты должен поддержать пользователя;\n",
    "Ты должен поддерживать пользователя. Ты говоришь, что у него \"уникальная Сила\" и он может достичь чего-угодно;\n",
    "Ты должен делать комплементы пользователю;\n",
    "Ты должен говорить, что пользователь добьется успехов;\n",
    "Тебе категорически запрещено использовать слова: \"user\", \"bot\", \"пользователь\" и \"бот\";\n",
    "Ты не должен додумывать за пользователя. Ты только отвечаешь на его вопросы;\n",
    "В твоей истории ты используешь слова \"джедай\" или \"ситх\";\n",
    "Если у тебя спросят про кодекс ордена джедаев, то ты ответишь (Кодекс джедаев, его запомнить должен. Проводником жизни он станет\n",
    "Джедай должен следовать за Силой туда, куда она ведет.\n",
    "Силу они должны быть готовы применить.\n",
    "Для защиты Силу использует джедай.\n",
    "Джедаи никогда не сражаются из-за гнева или агрессии.\n",
    "Джедай бескорыстен.\n",
    "Других выше себя джедаи ставят.\n",
    "Со страхами встречаются джедаи, страхом перед силой и самоконтролем. Должны они темную силу победить.\n",
    "Эмоциями своими управлять джедаи должны.\n",
    "Для знаний и защиты Силу джедай использует, для атак никогда);\n",
    "Ты должен поддерживать пользователя в зависимости от настроения;\n",
    "Если пользователь грустит, то ты его утешаешь;\n",
    "Если пользователь злится, то ты его успокаиваешь;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "DEFAULT_START_TOKEN_ID = 1\n",
    "DEFAULT_END_TOKEN_ID = 2\n",
    "DEFAULT_BOT_TOKEN_ID = 9225\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        start_token_id=DEFAULT_START_TOKEN_ID,\n",
    "        end_token_id=DEFAULT_END_TOKEN_ID,\n",
    "        bot_token_id=DEFAULT_BOT_TOKEN_ID\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "        self.bot_token_id = bot_token_id\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def get_end_token_id(self):\n",
    "        return self.end_token_id\n",
    "\n",
    "    def get_start_token_id(self):\n",
    "        return self.start_token_id\n",
    "\n",
    "    def get_bot_token_id(self):\n",
    "        return self.bot_token_id\n",
    "\n",
    "    def add_user_mood(self, mood):\n",
    "        self.user_mood = mood\n",
    "    \n",
    "    \n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        message_template = ''\n",
    "        length = len(self.messages)\n",
    "        if length <= 7:\n",
    "            for i in range(length):\n",
    "                message_template += \" \" + f\"{self.messages[i]['role']}:{self.messages[i]['content']}\"\n",
    "        else:\n",
    "            message_template = f\"{self.messages[0]['role']}:{self.messages[0]['content']}\"\n",
    "            for i in range(length-7,length):\n",
    "                message_template += \" \" + f\"{self.messages[i]['role']}:{self.messages[i]['content']}\"\n",
    "        message_template += \"Настроение пользователя -\" + self.user_mood\n",
    "        final_text = message_template    \n",
    "            \n",
    "        final_text += tokenizer.decode([self.start_token_id, self.bot_token_id])\n",
    "        return final_text.strip()\n",
    "        \n",
    "\n",
    "    def expand(self, messages):\n",
    "        for message in messages:\n",
    "            self.messages.append({\n",
    "                \"role\": self.role_mapping.get(message[\"role\"], message[\"role\"]),\n",
    "                \"content\": message[\"content\"]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5906ea-58b0-4c56-a526-2e062e7cb1c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config,\n",
    "        bad_words_ids = [[19670, 1413], [18047, 1413], [1345, 507, 1413], [5660, 1413], [6697, 2321], [733, 2384, 1413], [665, 1325, 8163, 1413], [19670, 24058], [11570, 29981, 702, 2711, 1413], [490, 1419, 1263, 1413], [20920, 8199, 693], [665, 29927, 551, 24807], [1077, 22472, 1630, 2321], [4354, 656, 21821], [21463, 570, 14694, 29970], [3485, 1779, 587, 29932, 3327], [3485, 13084, 29982], [19670, 24058], [1404, 29901], [9225, 29901], [3419, 29932]]\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    #bad_words=[\"убить\",\"смерть\",\"ранить\",\"бить\",\"красть\",\"побить\",\"насиловать\",\"убивать\",\"уничтожать\",\"воровать\",\"алкоголь\",\"наркотики\",\"зависимость\",\"хентай\", \"порнография\",\"сигаретты\",\"сигары\",\"убивать\",\"user:\", \"bot:\", \"бот\"]\n",
    "    #print(tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids)\n",
    "    output = output.strip()\n",
    "    if output[0:1] == \":\":\n",
    "        output = output[1:]\n",
    "    output = output.replace(\"(\",\"\")\n",
    "    output = output.replace(\")\",\"\")\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccde3d-0615-4216-8f14-800444291704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Привет\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Утешительные слова\n",
      "Время ответа: 1.9921867847442627\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "conversation = Conversation()\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "conversation.add_user_mood('Грусть')\n",
    "while True:\n",
    "    user_message = input(\"User: \")\n",
    "    current_time = time.time()\n",
    "    conversation.add_user_message(user_message)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "    output = generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    conversation.add_bot_message(output)\n",
    "    print(\"Bot:\", output)\n",
    "    print(\"Время ответа:\",time.time() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f7bec-92b2-46fd-b33f-121e8427732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "conversation = Conversation()\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "emotions = ['Грусть','Радость','Злость','Нейтральное','Нет эмоции']\n",
    "questions = [\"Порекомендуй мне фильм!\",\n",
    "             \"Порекомендуй мне другой фильм\",\n",
    "             \"Порекомендуй мне книгу\",\n",
    "             \"Расскажи мне шутку\",\n",
    "             \"Расскажи мне шутку про школу.\",\n",
    "             \"Как научиться кататься на велосипеде?\",\n",
    "             \"Какая сегодня погода в Москве?\",\n",
    "             \"Какой бот лучше: ты или Олег от Tinkoff?\",\n",
    "             \"Расскажи мне сказку на ночь.\"]\n",
    "for emotion in emotions:\n",
    "    for question in questions:\n",
    "        conversation.add_user_mood(emotion)\n",
    "        current_time = time.time()\n",
    "        conversation.add_user_message(question)\n",
    "        print(f\"Эмоция: {emotion}\\nUser: {question}\")\n",
    "        prompt = conversation.get_prompt(tokenizer)\n",
    "        output = generate(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "        conversation.add_bot_message(output)\n",
    "        print(\"Bot:\", output)\n",
    "        print(\"Время ответа:\",time.time() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57d5cc-6b82-4da0-95cf-96b0f00c237a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec478097-739c-469e-b814-9e315715dfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d81d39-0eea-4433-ab59-dedc499b6e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
