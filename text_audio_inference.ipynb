{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517b801b-268e-4145-806e-59f126d30d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "import torch\n",
    "from typing import List\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import Adafactor\n",
    "import torch.nn as nn\n",
    "from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6367d6-897d-41ab-b1eb-e3731e6ab3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-11 19:39:03--  https://huggingface.co/datasets/KELONMYOSA/dusha_emotion_audio/resolve/main/data/train.tar.gz\n",
      "Resolving huggingface.co (huggingface.co)... 99.84.108.129, 99.84.108.87, 99.84.108.70, ...\n",
      "Connecting to huggingface.co (huggingface.co)|99.84.108.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/f9/36/f936cc9a1834dea987df3d43bad12716beb9c022f2f51034d7372be499277925/906fac85c0d247cd5b98de94b2ed75fa7c1e0bba1e0361ae8413018b5eca3934?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27train.tar.gz%3B+filename%3D%22train.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1689363543&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTM2MzU0M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mOS8zNi9mOTM2Y2M5YTE4MzRkZWE5ODdkZjNkNDNiYWQxMjcxNmJlYjljMDIyZjJmNTEwMzRkNzM3MmJlNDk5Mjc3OTI1LzkwNmZhYzg1YzBkMjQ3Y2Q1Yjk4ZGU5NGIyZWQ3NWZhN2MxZTBiYmExZTAzNjFhZTg0MTMwMThiNWVjYTM5MzQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=PHHG20BXsyqJTOnVmCLoGha4w3TtXyB2%7E9-%7EzWE5FrHVmqBW7oB6kQXNBwCw7mZ74Wu8OCUjSCMaz400mbJas%7E4Zcnwmu3CCY6Hjezzgf2OabUNKuGX4QcrPN4Vlfdz6zSvQMv0ymW4thf2H86nVopMwxFU0R1QsE0siKGfbC55I907weVO5qVtBn5LbdjRva4TF9K3L18rfKmH5JOr2jyJq-WGr0ipmqroV%7ElWcB-ezPTqPs5Veh3IpWGOF2AJbpJzYoDQvJ6XvJq8g5lLEkn20oDTSlHVCevhIX%7EmQK1vRUwC3ZRiha6k1%7E-EIAOllBvo5SQfVs2UgCgrS788BXg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-07-11 19:39:03--  https://cdn-lfs.huggingface.co/repos/f9/36/f936cc9a1834dea987df3d43bad12716beb9c022f2f51034d7372be499277925/906fac85c0d247cd5b98de94b2ed75fa7c1e0bba1e0361ae8413018b5eca3934?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27train.tar.gz%3B+filename%3D%22train.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1689363543&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTM2MzU0M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mOS8zNi9mOTM2Y2M5YTE4MzRkZWE5ODdkZjNkNDNiYWQxMjcxNmJlYjljMDIyZjJmNTEwMzRkNzM3MmJlNDk5Mjc3OTI1LzkwNmZhYzg1YzBkMjQ3Y2Q1Yjk4ZGU5NGIyZWQ3NWZhN2MxZTBiYmExZTAzNjFhZTg0MTMwMThiNWVjYTM5MzQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=PHHG20BXsyqJTOnVmCLoGha4w3TtXyB2%7E9-%7EzWE5FrHVmqBW7oB6kQXNBwCw7mZ74Wu8OCUjSCMaz400mbJas%7E4Zcnwmu3CCY6Hjezzgf2OabUNKuGX4QcrPN4Vlfdz6zSvQMv0ymW4thf2H86nVopMwxFU0R1QsE0siKGfbC55I907weVO5qVtBn5LbdjRva4TF9K3L18rfKmH5JOr2jyJq-WGr0ipmqroV%7ElWcB-ezPTqPs5Veh3IpWGOF2AJbpJzYoDQvJ6XvJq8g5lLEkn20oDTSlHVCevhIX%7EmQK1vRUwC3ZRiha6k1%7E-EIAOllBvo5SQfVs2UgCgrS788BXg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.111, 108.138.64.49, 108.138.64.36, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.111|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9033469945 (8.4G) [application/gzip]\n",
      "Saving to: ‘train.tar.gz’\n",
      "\n",
      "train.tar.gz        100%[===================>]   8.41G  44.2MB/s    in 2m 59s  \n",
      "\n",
      "2023-07-11 19:42:03 (48.0 MB/s) - ‘train.tar.gz’ saved [9033469945/9033469945]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://huggingface.co/datasets/KELONMYOSA/dusha_emotion_audio/resolve/main/data/train.tar.gz --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92a20b-b34a-4985-8953-0a21f45971b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvzf train.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6e5c33-8247-43e0-b520-2218a28bfbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "a486378e-bdb4-47d5-b402-e16b2bf6951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = AutoModelForSequenceClassification.from_pretrained('./model/bert.ppt',ignore_mismatched_sizes=True, return_dict=True, num_labels=5).to(device)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('cointegrated/rubert-tiny2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "7843ae89-6c52-414a-abec-1331ebc42dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\n",
    "audio_model = HubertForSequenceClassification.from_pretrained(\"xbgoose/hubert-base-speech-emotion-recognition-russian-dusha-finetuned\", output_hidden_states=True)\n",
    "num2emotion = {0: 'neutral', 1: 'angry', 2: 'positive', 3: 'sad', 4: 'other'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "626fb813-db35-490c-8502-21de6efce685",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"train/84088ff0c9766472f1ba79d2e17e622b.wav\"\n",
    "\n",
    "# waveform, sample_rate = torchaudio.load(filepath, normalize=True)\n",
    "# transform = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "# waveform = transform(waveform)\n",
    "\n",
    "# inputs = feature_extractor(\n",
    "#         waveform, \n",
    "#         sampling_rate=feature_extractor.sampling_rate, \n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True,\n",
    "#         max_length=16000 * 10,\n",
    "#         truncation=True\n",
    "#     )\n",
    "\n",
    "# logits = model(inputs['input_values'][0]).logits\n",
    "# predictions = torch.argmax(logits, dim=-1)\n",
    "# predicted_emotion = num2emotion[predictions.numpy()[0]]\n",
    "# print(predicted_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee73c8-57cb-45b3-b302-3d0b335febbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "153b9794-2f4e-41db-8885-350bd7e34a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, text_model, audio_model):\n",
    "        super(Net, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.audio_model = audio_model\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(763, 763, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(763, 5, bias=True)\n",
    "        )\n",
    "\n",
    "        self.emotion2label = {\n",
    "            \"neutral\":0,\n",
    "            \"positive\":1,\n",
    "            \"sad\":2,\n",
    "            \"angry\":3,\n",
    "            \"other\":4\n",
    "        }\n",
    "\n",
    "    def tokenize_function(self, text):\n",
    "        tokenized = tokenizer(text, max_length=512, padding='max_length', return_tensors='pt')\n",
    "        return torch.cat([tokenized['input_ids'], \n",
    "                          tokenized['token_type_ids'], \n",
    "                          tokenized['attention_mask']], 0)\n",
    "\n",
    "    def forward(self, text, audio_path):\n",
    "        \n",
    "        text_data = self.tokenize_function(text).unsqueeze(dim=0)\n",
    "        b_input_ids, b_type_ids, b_input_mask,  = text_data[:, 0, :], text_data[:, 1, :], text_data[:, 2, :]\n",
    "        # print(b_input_ids)\n",
    "        logits_a = self.text_model(b_input_ids.cuda().long(),\n",
    "                       b_input_mask.cuda(),\n",
    "                       b_type_ids.cuda().long(),\n",
    "                       output_hidden_states=True\n",
    "                       # labels=one_hot_labels\n",
    "                      )['hidden_states'][-1][:, :, -1]#[:, 0, :])\n",
    "\n",
    "        # print(logits_a)\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(filepath, normalize=True)\n",
    "        transform = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = transform(waveform)\n",
    "        \n",
    "        inputs = feature_extractor(\n",
    "                waveform, \n",
    "                sampling_rate=feature_extractor.sampling_rate, \n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                max_length=16000 * 10,\n",
    "                truncation=True\n",
    "            )\n",
    "        \n",
    "        logits_b = self.audio_model(inputs['input_values'][0].cuda())['hidden_states'][-1][:, :, -1]\n",
    "        # print(logits_b)\n",
    "        concatenated_vectors = torch.cat([logits_a.cuda(), logits_b.cuda()], 1)\n",
    "        output = self.classifier(concatenated_vectors)\n",
    "        return output\n",
    "        # return concatenated_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "44caa6b6-7eac-4649-bcdd-7bfcf767d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(text_model, audio_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "b579ea8f-ecc7-403c-9ba0-8723869fa18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (text_model): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "        (position_embeddings): Embedding(2048, 312)\n",
       "        (token_type_embeddings): Embedding(2, 312)\n",
       "        (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-2): 3 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "                (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=312, out_features=5, bias=True)\n",
       "  )\n",
       "  (audio_model): HubertForSequenceClassification(\n",
       "    (hubert): HubertModel(\n",
       "      (feature_extractor): HubertFeatureEncoder(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): HubertGroupNormConvLayer(\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "            (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1-4): 4 x HubertNoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (5-6): 2 x HubertNoLayerNormConvLayer(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feature_projection): HubertFeatureProjection(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): HubertEncoder(\n",
       "        (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "          (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "          (padding): HubertSamePadLayer()\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x HubertEncoderLayer(\n",
       "            (attention): HubertAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward): HubertFeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projector): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (classifier): Linear(in_features=256, out_features=5, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=763, out_features=763, bias=True)\n",
       "    (2): Tanh()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=763, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "f7d95dfb-4d85-4a65-a01c-2acf1864e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.1.weight\n",
      "classifier.1.bias\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'text_model' in name or 'audio_model' in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(name)\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f03317ed-33cf-4326-83ce-e9f61b182e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2emotion = {\n",
    "    0: \"neutral\",\n",
    "    1: \"positive\",\n",
    "    2: \"sad\",\n",
    "    3: \"angry\",\n",
    "    4: \"other\"\n",
    "}\n",
    "emotion2label = {\n",
    "            \"neutral\":0,\n",
    "            \"positive\":1,\n",
    "            \"sad\":2,\n",
    "            \"angry\":3,\n",
    "            \"other\":4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "e27f04e8-f89c-4c27-b28b-e2f1e3368523",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dataframe = pd.read_csv('train_dataset.csv')\n",
    "train_text_frame = pd.read_csv('train_with_text.csv')\n",
    "train_text_frame.rename(columns = {'audio_path':'file'}, inplace=True)\n",
    "train_text_frame.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "train_dataset_dataframe.rename(columns = {'audio_path':'file'}, inplace=True)\n",
    "train_frame = train_text_frame.merge(train_dataset_dataframe, how='left', on='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "9cc533e1-c69d-4df4-a6e1-4d361e526cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>predicted_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/56034f672277aa3aa4c0d5fec976e25f.wav</td>\n",
       "      <td>ты сейчас не говорила</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/84088ff0c9766472f1ba79d2e17e622b.wav</td>\n",
       "      <td>воровайки первый концерт</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/3cf659c3c384271cb22c45297f0f0693.wav</td>\n",
       "      <td>спасибо приятно получать комплимент</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/a9308e85c5cdd1731ed395cef1a226ed.wav</td>\n",
       "      <td>что находится</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/d3bd2f6258e44b2c1283d2d7526f4fc9.wav</td>\n",
       "      <td>фильм пассажир в хорошем качестве</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158411</th>\n",
       "      <td>train/895b299e0a21323dffdd0eb0dfa1345c.wav</td>\n",
       "      <td>пгт смирных аэродром</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158412</th>\n",
       "      <td>train/b5174074a9b257251ab0c026bbd581f4.wav</td>\n",
       "      <td>пузырек своими руками</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158413</th>\n",
       "      <td>train/b5174074a9b257251ab0c026bbd581f4.wav</td>\n",
       "      <td>пузырек своими руками</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158414</th>\n",
       "      <td>train/b28cfb2bfc0482cb53c3c9aa09a1df5b.wav</td>\n",
       "      <td>смотреть фильм хэлловин хиби</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158415</th>\n",
       "      <td>train/b28cfb2bfc0482cb53c3c9aa09a1df5b.wav</td>\n",
       "      <td>смотреть фильм хэлловин хиби</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158416 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file  \\\n",
       "0       train/56034f672277aa3aa4c0d5fec976e25f.wav   \n",
       "1       train/84088ff0c9766472f1ba79d2e17e622b.wav   \n",
       "2       train/3cf659c3c384271cb22c45297f0f0693.wav   \n",
       "3       train/a9308e85c5cdd1731ed395cef1a226ed.wav   \n",
       "4       train/d3bd2f6258e44b2c1283d2d7526f4fc9.wav   \n",
       "...                                            ...   \n",
       "158411  train/895b299e0a21323dffdd0eb0dfa1345c.wav   \n",
       "158412  train/b5174074a9b257251ab0c026bbd581f4.wav   \n",
       "158413  train/b5174074a9b257251ab0c026bbd581f4.wav   \n",
       "158414  train/b28cfb2bfc0482cb53c3c9aa09a1df5b.wav   \n",
       "158415  train/b28cfb2bfc0482cb53c3c9aa09a1df5b.wav   \n",
       "\n",
       "                             predicted_text    label  \n",
       "0                     ты сейчас не говорила  neutral  \n",
       "1                  воровайки первый концерт  neutral  \n",
       "2       спасибо приятно получать комплимент  neutral  \n",
       "3                             что находится  neutral  \n",
       "4         фильм пассажир в хорошем качестве  neutral  \n",
       "...                                     ...      ...  \n",
       "158411                 пгт смирных аэродром  neutral  \n",
       "158412                пузырек своими руками  neutral  \n",
       "158413                пузырек своими руками  neutral  \n",
       "158414         смотреть фильм хэлловин хиби  neutral  \n",
       "158415         смотреть фильм хэлловин хиби  neutral  \n",
       "\n",
       "[158416 rows x 3 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "caf0c638-c0a4-4a8c-8a5a-51adda02794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = train_frame.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "7fa15420-5f4a-4659-949b-7a0d08d9a710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('фильм пассажир в хорошем качестве',\n",
       " 'train/d3bd2f6258e44b2c1283d2d7526f4fc9.wav')"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['predicted_text'], test_data['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "db6f0dfe-4af2-4afb-bcc2-c04fe1cd2a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4182,  0.0324,  0.2856, -0.4516, -0.5012]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    test_data['predicted_text'], test_data['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "cd32e2a9-13fa-47f1-b4d3-736248df48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 5e-5)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "10c443da-659d-4dc7-a86a-a2d92abefe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "0d1238da-59d5-4b5d-a83c-2dcbeee52ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "5662f268-6cef-4900-b669-16c68b52d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "6419b79a-f081-4c6c-817e-66561eba038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pkl', 'rb') as file:\n",
    "    dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "6a09feaf-5a03-4781-9cef-1f949f330290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d51421883346b6aa8cb672403f789a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0450e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.4397e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1471e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1603e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6358e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4928e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8504e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.4754e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9815e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0265e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6106e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5153e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0981e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5139e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4305e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.4754e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(9.6674e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7550e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8146e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4080e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7179e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4424e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5259e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6332e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5102e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2040e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.5830e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8120e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.6757e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3644e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.5565e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6955e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8876e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9802e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6955e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8610e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5139e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8610e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1206e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9457e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4093e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0252e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2636e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.5830e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0133e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1723e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1989e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1260e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3644e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8147e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1458e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5034e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6689e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6689e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9073e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1921e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1458e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3113e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.3446e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7684e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4305e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.3446e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7684e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3842e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(17.9327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8636e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2517e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2755e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5497e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3181e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0610e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1563e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1525e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2452e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.9407e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0797e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2452e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.5299e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1989e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2915e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.5299e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0266e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2186e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0994e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2650e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5034e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6226e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3842e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5497e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4305e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0729e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7881e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(6.5565e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5102e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2452e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0599e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3379e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2650e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0266e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7881e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5034e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3113e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4305e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1526e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3113e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1921e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(9.5367e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2452e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1921e-06, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1526e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(8.3446e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9605e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7684e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5763e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1526e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5763e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3842e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1526e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3842e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3842e-07, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for text, audio_path, text_label in tqdm(zip(dataset['predicted_text'], dataset['file'], dataset['label']), total=len(dataset)):\n",
    "        optimizer.zero_grad()\n",
    "        label = torch.tensor(emotion2label[text_label]).cuda()\n",
    "        output = model(text, audio_path)\n",
    "        loss_ = loss(output, torch.unsqueeze(label, 0))\n",
    "        loss_array.append(loss_)\n",
    "        if i % 300 == 0:\n",
    "            print(loss_)\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "fab5e249-8e7f-40f2-a6b1-407e2638de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dataframe = pd.read_csv('test.csv')\n",
    "dataset_dataframe.columns = ['file', 'label']\n",
    "dataset_dataframe['emotion'] = dataset_dataframe['label'].replace(label2emotion)\n",
    "text_frame = pd.read_csv('test_with_text.csv')\n",
    "text_frame.rename(columns={'audio_path':'file'}, inplace=True)\n",
    "text_frame.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "dataset_dataframe = dataset_dataframe.merge(text_frame, how='left', on='file').drop(columns=['emotion'])\n",
    "dataset_dataframe['predicted_text'].fillna(' ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "f3fb16f0-1d99-4c42-b692-660b744ed150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6859bfc1905426386a1575e1474cbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "i = 0\n",
    "for text, audio_path, text_label in tqdm(zip(dataset_dataframe['predicted_text'], dataset_dataframe['file'], dataset_dataframe['label']), total=len(dataset_dataframe)):\n",
    "    label = torch.tensor(emotion2label[text_label]).cpu()\n",
    "    output = model(text, audio_path)\n",
    "    pred_labels.append(torch.argmax(output).cpu())\n",
    "    true_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "5480375e-6fbf-4780-a92f-7c811d14fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     15886\n",
      "           1       0.02      0.04      0.02      2481\n",
      "           2       0.00      0.00      0.00      2506\n",
      "           3       0.00      0.00      0.00      3072\n",
      "           4       0.01      0.96      0.02       226\n",
      "\n",
      "    accuracy                           0.01     24171\n",
      "   macro avg       0.01      0.20      0.01     24171\n",
      "weighted avg       0.00      0.01      0.00     24171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "c4899962-e78d-4575-b9da-635689e36580",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [int(x) for x in true_labels]\n",
    "pred_labels = [int(x) for x in pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "5387ae08-4c75-4274-a8a5-f250970f0960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     15886\n",
      "           1       0.02      0.04      0.02      2481\n",
      "           2       0.00      0.00      0.00      2506\n",
      "           3       0.00      0.00      0.00      3072\n",
      "           4       0.01      0.96      0.02       226\n",
      "\n",
      "    accuracy                           0.01     24171\n",
      "   macro avg       0.01      0.20      0.01     24171\n",
      "weighted avg       0.00      0.01      0.00     24171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca142345-310c-49ac-85e3-dfd3dd919cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
